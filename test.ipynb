{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.signal import find_peaks, butter, filtfilt\n",
    "from statsmodels import api\n",
    "def lowpass_filter(signal, cutoff_freq, fs, order=5):\n",
    "\t# lowpass filter\n",
    "\tb, a = butter(order, cutoff_freq / (fs / 2), btype='lowpass')\n",
    "\tfiltered_signal = filtfilt(b, a, signal)\n",
    "\n",
    "\treturn filtered_signal\n",
    "\n",
    "\n",
    "class HumanPrescenceClassifier:\n",
    "    def __init__(self, snr_threshold, motion_threshold, prominence=0.011, width=70, sampling_freq=100):\n",
    "        self.snr_threshold = snr_threshold\n",
    "        self.motion_threshold = motion_threshold\n",
    "        self.prominence = prominence\n",
    "        self.width = width\n",
    "        self.sampling_freq = sampling_freq\n",
    "\n",
    "\n",
    "    def maximum_ratio_combining(self):\n",
    "        \"Return weights under Maximum Ratio Combining\"\n",
    "        # use Maximum Ratio Combining\n",
    "        mss = []\n",
    "        # calculate the motion statistics from all subcarriers\n",
    "        for i in range(self.subcarrier_power.shape[1]):\n",
    "            CSI_amplitude = self.subcarrier_power[:, i]\n",
    "            autocorr = api.tsa.stattools.acf(CSI_amplitude, nlags=100 - 1)\n",
    "            mss.append(autocorr[1])\n",
    "\n",
    "        # use Maximum Ratio Combining\n",
    "        mss = np.array(mss)\n",
    "        mss = mss / np.sum(mss)\n",
    "        return mss\n",
    "    \n",
    "\n",
    "    def mrc_csi(self):\n",
    "        weights = self.maximum_ratio_combining()\n",
    "        CSI_full = self.subcarrier_power * weights\n",
    "        CSI_full = np.sum(CSI_full, axis=1)\n",
    "        return CSI_full\n",
    "    \n",
    "    def motion_detection_score_sub1(self, subcarrier=1):\n",
    "        CSI_amplitude = self.subcarrier_power[:, subcarrier]\n",
    "        \n",
    "        autocorr = api.tsa.stattools.acf(CSI_amplitude, nlags=1)\n",
    "        motion_statistics = autocorr[1]\n",
    "        return motion_statistics\n",
    "\n",
    "    \n",
    "    def preprocess(self, csi_data):\n",
    "        \"\"\"csi_data not processed\"\"\"\n",
    "\n",
    "        # Identify the subcarriers to prune (e.g., subcarriers with all 0 values)\n",
    "        subcarriers_to_prune = np.where(~csi_data.any(axis=0))[0]\n",
    "\n",
    "        # Remove the subcarriers from the data\n",
    "        pruned_csi_data = np.delete(csi_data, subcarriers_to_prune, axis=1)\n",
    "\n",
    "        # print(\"Original CSI data size:\", csi_data.shape)\n",
    "        # print(\"Pruned CSI data size:\", pruned_csi_data.shape)\n",
    "\n",
    "        csi_data = pruned_csi_data\n",
    "\n",
    "        if csi_data.shape[1] == 0:\n",
    "            raise ValueError(\"No subcarriers left after pruning\")\n",
    "\n",
    "        # Compute the power of each subcarrier\n",
    "        subcarrier_power = np.square(np.abs(csi_data))\n",
    "\n",
    "        # Compute the noise power by averaging the power over all samples\n",
    "        noise_power = np.mean(subcarrier_power, axis=1)\n",
    "\n",
    "        # Compute the SNR of each subcarrier by dividing its power by the noise power\n",
    "        subcarrier_snr = subcarrier_power / noise_power[:, np.newaxis]\n",
    "\n",
    "        # # Set a threshold for the minimum SNR value\n",
    "        # snr_threshold = 0.01\n",
    "\n",
    "        # Find the subcarriers with SNR values above the threshold\n",
    "        good_subcarriers = np.where(np.all(subcarrier_snr >= self.snr_threshold, axis=0))[0]\n",
    "\n",
    "        if len(good_subcarriers) == 0:\n",
    "            self.subcarrier_power = subcarrier_power\n",
    "            return np.argmax(np.mean(subcarrier_snr, axis=0))\n",
    "\n",
    "        # Filter out the subcarriers with low SNR\n",
    "        filtered_csi_data = subcarrier_power[:, good_subcarriers]\n",
    "\n",
    "        # find the best subcarrier\n",
    "        best_subcarrier = np.argmax(np.mean(subcarrier_snr[:, good_subcarriers], axis=0))\n",
    "\n",
    "        # print(\"Best subcarrier:\", best_subcarrier)\n",
    "        \n",
    "        # print(\"Original CSI data size:\", subcarrier_power.shape)\n",
    "        # print(\"Filtered CSI data size:\", filtered_csi_data.shape)\n",
    "        self.subcarrier_power = filtered_csi_data\n",
    "\n",
    "        return best_subcarrier\n",
    "\n",
    "    def predict(self, csi_data):\n",
    "        # Filter out the subcarriers with low SNR\n",
    "        best_subcarrier = self.preprocess(csi_data)\n",
    "        # print(\"Best subcarrier:\", best_subcarrier)\n",
    "\n",
    "        # # Compute the motion statistics for each subcarrier\n",
    "        # motion_statistics = motion_detection_score_all(filtered_CSI_amplitude)\n",
    "\n",
    "        # # Find the subcarrier with the highest motion statistics\n",
    "        # best_subcarrier = np.argmax(motion_statistics)\n",
    "\n",
    "        # Compute the motion statistics for the best subcarrier\n",
    "        motion_statistics = self.motion_detection_score_sub1(subcarrier=best_subcarrier)\n",
    "\n",
    "        if motion_statistics >= self.motion_threshold:\n",
    "            return True\n",
    "        \n",
    "        \n",
    "        autocorr = lowpass_filter(api.tsa.stattools.acf(self.mrc_csi(), nlags=700 - 1)[1:], 1, self.sampling_freq)\n",
    "\n",
    "        autocorr *= -1\n",
    "        # Find the peaks in the motion statistics\n",
    "        peaks, _ = find_peaks(autocorr, prominence=self.prominence, width=self.width)\n",
    "\n",
    "        return len(peaks) >= 2\n",
    "    \n",
    "    \n",
    "def parse_data(data_string):\n",
    "    return data_string[1:-1].split(',')\n",
    "\n",
    "def parse_data2(data_string):\n",
    "    return data_string[1:-1].split(', ')\n",
    "\n",
    "def parse_csi_data(data):\n",
    "    return [complex(int(data[i]), int(data[i+1])) for i in range(0, len(data), 2)]\n",
    "\n",
    "def parse_csi_data2(data):\n",
    "    return [complex(int(data[i][1:-1]), int(data[i+1][1:-1])) for i in range(0, len(data), 2)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = HumanPrescenceClassifier(snr_threshold=0.009, motion_threshold=0.1, prominence=0.011, width=70, sampling_freq=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      "----\n",
      "----\n",
      "----\n",
      "----\n",
      "----\n",
      "----\n",
      "----\n",
      "----\n",
      "(39, 1000, 64)\n",
      "----\n",
      "----\n",
      "----\n",
      "(11, 1000, 64)\n",
      "(50, 1000, 64)\n",
      "(50,)\n"
     ]
    }
   ],
   "source": [
    "# combine the data\n",
    "file_list_positive = ['sitting/2.csv', 'sitting/3.csv', 'sitting/4.csv', 'sitting/csi_data.csv', 'walking/1.csv', 'walking/2.csv', 'walking/3.csv', 'walking/4.csv', 'walking/5.csv'], (parse_data, parse_csi_data), True\n",
    "file_list_negative = ['no one/2.csv', 'no one/3.csv', 'no one/csi_data.csv'], (parse_data2, parse_csi_data2), False\n",
    "CSI_data_list = []\n",
    "labels = []\n",
    "\n",
    "window_size = 1000\n",
    "step_size = 500\n",
    "\n",
    "for file_list, (f1, f2), label in file_list_positive, file_list_negative:\n",
    "    data = []\n",
    "    for file in file_list:\n",
    "        df = pd.read_csv(file)\n",
    "        df['data'] = df['data'].apply(f1)\n",
    "        CSI_data = np.array(df['data'].apply(f2).to_list())\n",
    "\n",
    "        for i in range(window_size, CSI_data.shape[0], step_size):\n",
    "            # get the window data (last window size data)\n",
    "            window_CSI_data = CSI_data[max(0, i-window_size):i, :]\n",
    "            \n",
    "            # predict\n",
    "            data.append(window_CSI_data)\n",
    "        print(\"----\")\n",
    "\n",
    "    data = np.array(data)\n",
    "    CSI_data_list.append(data)\n",
    "    print(data.shape)\n",
    "    # labels.append(np.ones(CSI_data.shape[0]) * label)\n",
    "    labels.append(np.full((data.shape[0],), label, dtype=bool))\n",
    "\n",
    "\n",
    "CSI_data = np.concatenate(CSI_data_list, axis=0)\n",
    "print(CSI_data.shape)\n",
    "labels = np.concatenate(labels, axis=0)\n",
    "print(labels.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False, False, False, False, False, False, False, False,\n",
       "       False])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "# Split the CSI data into a training set and a validation set\n",
    "X_train, X_val, y_train, y_val = train_test_split(CSI_data, labels, test_size=.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15, 1000, 64)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15, 1000, 64)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def report_metrics(y_true, model, X):\n",
    "    # predict valid set\n",
    "    predictions = []\n",
    "\n",
    "    for i in range(X.shape[0]):\n",
    "        # get the window data (last window size data)\n",
    "        window_CSI_data = X[i]\n",
    "        \n",
    "        # predict\n",
    "        predictions.append(model.predict(window_CSI_data))\n",
    "\n",
    "    # report the tpr, fpr, tnr, fnr\n",
    "    print(\"TPR:\", np.sum(np.logical_and(predictions, y_true)) / np.sum(y_true))\n",
    "    print(\"FPR:\", np.sum(np.logical_and(predictions, np.logical_not(y_true))) / np.sum(np.logical_not(y_true)))\n",
    "    print(\"TNR:\", np.sum(np.logical_and(np.logical_not(predictions), np.logical_not(y_true))) / np.sum(np.logical_not(y_true)))\n",
    "    print(\"FNR:\", np.sum(np.logical_and(np.logical_not(predictions), y_true)) / np.sum(y_true))\n",
    "\n",
    "    print(\"Accuracy:\", accuracy_score(y_true, predictions))\n",
    "    print(\"Precision:\", precision_score(y_true, predictions))\n",
    "    print(\"Recall:\", recall_score(y_true, predictions))\n",
    "    print(\"F1:\", f1_score(y_true, predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = HumanPrescenceClassifier(snr_threshold=0.1, motion_threshold=0.1, prominence=0.011, width=70, sampling_freq=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TPR: 0.9166666666666666\n",
      "FPR: 0.0\n",
      "TNR: 1.0\n",
      "FNR: 0.08333333333333333\n",
      "Accuracy: 0.9333333333333333\n",
      "Precision: 1.0\n",
      "Recall: 0.9166666666666666\n",
      "F1: 0.9565217391304348\n"
     ]
    }
   ],
   "source": [
    "report_metrics(y_val, classifier, X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "classifier = HumanPrescenceClassifier(**{'motion_threshold': 0.27507400608839916, 'prominence': 0.021202887303199747, 'snr_threshold': 0.035990422909134456, 'width': 34.657920510962626})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TPR: 0.9166666666666666\n",
      "FPR: 0.0\n",
      "TNR: 1.0\n",
      "FNR: 0.08333333333333333\n",
      "Accuracy: 0.9333333333333333\n",
      "Precision: 1.0\n",
      "Recall: 0.9166666666666666\n",
      "F1: 0.9565217391304348\n"
     ]
    }
   ],
   "source": [
    "report_metrics(y_val, classifier, X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_classifier = HumanPrescenceClassifier(snr_threshold=0.7, motion_threshold=0.1, prominence=0.011, width=70, sampling_freq=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TPR: 1.0\n",
      "FPR: 0.0\n",
      "TNR: 1.0\n",
      "FNR: 0.0\n",
      "Accuracy: 1.0\n",
      "Precision: 1.0\n",
      "Recall: 1.0\n",
      "F1: 1.0\n"
     ]
    }
   ],
   "source": [
    "report_metrics(y_val, best_classifier, X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TPR: 1.0\n",
      "FPR: 0.0\n",
      "TNR: 1.0\n",
      "FNR: 0.0\n",
      "Accuracy: 1.0\n",
      "Precision: 1.0\n",
      "Recall: 1.0\n",
      "F1: 1.0\n"
     ]
    }
   ],
   "source": [
    "report_metrics(labels, best_classifier, CSI_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | motion... | promin... | snr_th... |   width   |\n",
      "-------------------------------------------------------------------------\n",
      "| \u001b[0m1        \u001b[0m | \u001b[0m0.8261   \u001b[0m | \u001b[0m0.417    \u001b[0m | \u001b[0m0.7203   \u001b[0m | \u001b[0m0.0003431\u001b[0m | \u001b[0m30.23    \u001b[0m |\n",
      "| \u001b[95m2        \u001b[0m | \u001b[95m0.875    \u001b[0m | \u001b[95m0.1468   \u001b[0m | \u001b[95m0.09234  \u001b[0m | \u001b[95m0.5588   \u001b[0m | \u001b[95m34.56    \u001b[0m |\n",
      "| \u001b[0m3        \u001b[0m | \u001b[0m0.8511   \u001b[0m | \u001b[0m0.3968   \u001b[0m | \u001b[0m0.5388   \u001b[0m | \u001b[0m1.258    \u001b[0m | \u001b[0m68.52    \u001b[0m |\n",
      "| \u001b[0m4        \u001b[0m | \u001b[0m0.875    \u001b[0m | \u001b[0m0.2045   \u001b[0m | \u001b[0m0.8781   \u001b[0m | \u001b[0m0.08216  \u001b[0m | \u001b[0m67.05    \u001b[0m |\n",
      "| \u001b[0m5        \u001b[0m | \u001b[0m0.8261   \u001b[0m | \u001b[0m0.4173   \u001b[0m | \u001b[0m0.5587   \u001b[0m | \u001b[0m0.4212   \u001b[0m | \u001b[0m19.81    \u001b[0m |\n",
      "| \u001b[0m6        \u001b[0m | \u001b[0m0.8261   \u001b[0m | \u001b[0m0.4366   \u001b[0m | \u001b[0m0.5971   \u001b[0m | \u001b[0m1.137    \u001b[0m | \u001b[0m34.97    \u001b[0m |\n",
      "| \u001b[0m7        \u001b[0m | \u001b[0m0.875    \u001b[0m | \u001b[0m0.251    \u001b[0m | \u001b[0m0.581    \u001b[0m | \u001b[0m1.25     \u001b[0m | \u001b[0m68.62    \u001b[0m |\n",
      "| \u001b[0m8        \u001b[0m | \u001b[0m0.875    \u001b[0m | \u001b[0m0.2138   \u001b[0m | \u001b[0m0.8965   \u001b[0m | \u001b[0m0.1722   \u001b[0m | \u001b[0m67.1     \u001b[0m |\n",
      "| \u001b[95m9        \u001b[0m | \u001b[95m0.8814   \u001b[0m | \u001b[95m0.02121  \u001b[0m | \u001b[95m0.8079   \u001b[0m | \u001b[95m2.895    \u001b[0m | \u001b[95m70.21    \u001b[0m |\n",
      "| \u001b[0m10       \u001b[0m | \u001b[0m0.875    \u001b[0m | \u001b[0m0.2982   \u001b[0m | \u001b[0m0.9018   \u001b[0m | \u001b[0m2.816    \u001b[0m | \u001b[0m70.47    \u001b[0m |\n",
      "| \u001b[0m11       \u001b[0m | \u001b[0m0.875    \u001b[0m | \u001b[0m0.21     \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m2.662    \u001b[0m | \u001b[0m70.06    \u001b[0m |\n",
      "| \u001b[0m12       \u001b[0m | \u001b[0m0.875    \u001b[0m | \u001b[0m0.1377   \u001b[0m | \u001b[0m0.5607   \u001b[0m | \u001b[0m1.004    \u001b[0m | \u001b[0m68.96    \u001b[0m |\n",
      "| \u001b[0m13       \u001b[0m | \u001b[0m0.8511   \u001b[0m | \u001b[0m0.3249   \u001b[0m | \u001b[0m0.6358   \u001b[0m | \u001b[0m1.202    \u001b[0m | \u001b[0m75.16    \u001b[0m |\n",
      "| \u001b[0m14       \u001b[0m | \u001b[0m0.871    \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m0.7938   \u001b[0m | \u001b[0m1.384    \u001b[0m | \u001b[0m68.82    \u001b[0m |\n",
      "| \u001b[0m15       \u001b[0m | \u001b[0m0.875    \u001b[0m | \u001b[0m0.1161   \u001b[0m | \u001b[0m0.9551   \u001b[0m | \u001b[0m0.2566   \u001b[0m | \u001b[0m67.08    \u001b[0m |\n",
      "=========================================================================\n",
      "Optimal thresholds: {'motion_threshold': 0.021206273652628838, 'prominence': 0.8078812396267212, 'snr_threshold': 2.8945666233095646, 'width': 70.20730272435955}\n",
      "Maximum f1_score score: 0.8813559322033898\n",
      "Validation f1_score: 0.923076923076923\n"
     ]
    }
   ],
   "source": [
    "# Define the function to optimize, which takes two threshold values and returns the F1 score\n",
    "def objective(snr_threshold, motion_threshold, prominence, width):\n",
    "    predictions = []\n",
    "    classifier = HumanPrescenceClassifier(snr_threshold=snr_threshold, motion_threshold=motion_threshold, prominence=prominence, width=width, sampling_freq=100)\n",
    "    for i in range(X_train.shape[0]):\n",
    "        # get the window data\n",
    "        window_CSI_data = X_train[i]\n",
    "        \n",
    "        # predict\n",
    "        predictions.append(classifier.predict(window_CSI_data))\n",
    "\n",
    "    score = f1_score(y_train, predictions)  # Compute the F1 score\n",
    "    return score  # Minimize the negative F1 score\n",
    "\n",
    "# Set the bounds of the search space for the thresholds\n",
    "pbounds = {'snr_threshold': (0, 3), 'motion_threshold': (0, 1), 'prominence': (0, 1), 'width': (0, 100)}\n",
    "\n",
    "# Use a Bayesian optimization algorithm to find the optimal thresholds\n",
    "optimizer = BayesianOptimization(\n",
    "    f=objective,\n",
    "    pbounds=pbounds,\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "optimizer.maximize(\n",
    "    init_points=5,\n",
    "    n_iter=10,\n",
    ")\n",
    "\n",
    "# Print the optimal thresholds and the maximum F1 score\n",
    "print('Optimal thresholds:', optimizer.max['params'])\n",
    "print('Maximum f1_score score:', optimizer.max['target'])\n",
    "classifier = HumanPrescenceClassifier(**optimizer.max['params'])\n",
    "# predict valid set\n",
    "predictions = []\n",
    "\n",
    "for i in range(X_val.shape[0]):\n",
    "    # get the window data (last window size data)\n",
    "    window_CSI_data = X_val[i]\n",
    "    \n",
    "    # predict\n",
    "    predictions.append(classifier.predict(window_CSI_data))\n",
    "print(\"Validation f1_score:\", f1_score(y_val, predictions))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
