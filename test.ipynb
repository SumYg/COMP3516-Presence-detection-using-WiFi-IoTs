{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.signal import find_peaks, butter, filtfilt\n",
    "from statsmodels import api\n",
    "def lowpass_filter(signal, cutoff_freq, fs, order=5):\n",
    "\t# lowpass filter\n",
    "\tb, a = butter(order, cutoff_freq / (fs / 2), btype='lowpass')\n",
    "\tfiltered_signal = filtfilt(b, a, signal)\n",
    "\n",
    "\treturn filtered_signal\n",
    "\n",
    "\n",
    "class HumanPrescenceClassifier:\n",
    "    def __init__(self, snr_threshold, motion_threshold, prominence=0.011, width=70, sampling_freq=100):\n",
    "        self.snr_threshold = snr_threshold\n",
    "        self.motion_threshold = motion_threshold\n",
    "        self.prominence = prominence\n",
    "        self.width = width\n",
    "        self.sampling_freq = sampling_freq\n",
    "\n",
    "\n",
    "    def maximum_ratio_combining(self):\n",
    "        \"Return weights under Maximum Ratio Combining\"\n",
    "        # use Maximum Ratio Combining\n",
    "        mss = []\n",
    "        # calculate the motion statistics from all subcarriers\n",
    "        for i in range(self.subcarrier_power.shape[1]):\n",
    "            CSI_amplitude = self.subcarrier_power[:, i]\n",
    "            autocorr = api.tsa.stattools.acf(CSI_amplitude, nlags=100 - 1)\n",
    "            mss.append(autocorr[1])\n",
    "\n",
    "        # use Maximum Ratio Combining\n",
    "        mss = np.array(mss)\n",
    "        mss = mss / np.sum(mss)\n",
    "        return mss\n",
    "    \n",
    "\n",
    "    def mrc_csi(self):\n",
    "        weights = self.maximum_ratio_combining()\n",
    "        CSI_full = self.subcarrier_power * weights\n",
    "        CSI_full = np.sum(CSI_full, axis=1)\n",
    "        return CSI_full\n",
    "    \n",
    "    def motion_detection_score_sub1(self, subcarrier=1):\n",
    "        # CSI_amplitude = np.abs(CSI_raw)\n",
    "        # CSI_amplitude = gaussian_filter1d(CSI_amplitude, 3)**2\n",
    "        CSI_amplitude = self.subcarrier_power[:, subcarrier]\n",
    "        \n",
    "        autocorr = api.tsa.stattools.acf(CSI_amplitude, nlags=500 - 1)\n",
    "        motion_statistics = autocorr[1]\n",
    "        return motion_statistics\n",
    "\n",
    "    \n",
    "    def preprocess(self, csi_data):\n",
    "        \"\"\"csi_data not processed\"\"\"\n",
    "        # Assume that the CSI data is stored in a 2D numpy array called csi_data,\n",
    "        # where each row represents a time sample and each column represents a subcarrier.\n",
    "\n",
    "        # Identify the subcarriers to prune (e.g., subcarriers with all 0 values)\n",
    "        subcarriers_to_prune = np.where(~csi_data.any(axis=0))[0]\n",
    "\n",
    "        # Remove the subcarriers from the data\n",
    "        pruned_csi_data = np.delete(csi_data, subcarriers_to_prune, axis=1)\n",
    "\n",
    "        # Verify the results\n",
    "        # print(\"Original CSI data size:\", csi_data.shape)\n",
    "        # print(\"Pruned CSI data size:\", pruned_csi_data.shape)\n",
    "\n",
    "        csi_data = pruned_csi_data\n",
    "\n",
    "        if csi_data.shape[1] == 0:\n",
    "            raise ValueError(\"No subcarriers left after pruning\")\n",
    "\n",
    "\n",
    "        # Assume that the CSI data is stored in a numpy array called csi_data.\n",
    "        # The CSI data should be a 2D array with shape (num_samples, num_subcarriers).\n",
    "\n",
    "        # Compute the power of each subcarrier\n",
    "        subcarrier_power = np.square(np.abs(csi_data))\n",
    "\n",
    "        # Compute the noise power by averaging the power over all samples\n",
    "        noise_power = np.mean(subcarrier_power, axis=1)\n",
    "\n",
    "        # Compute the SNR of each subcarrier by dividing its power by the noise power\n",
    "        subcarrier_snr = subcarrier_power / noise_power[:, np.newaxis]\n",
    "\n",
    "        # # Set a threshold for the minimum SNR value\n",
    "        # snr_threshold = 0.01\n",
    "\n",
    "        # Find the subcarriers with SNR values above the threshold\n",
    "        good_subcarriers = np.where(np.all(subcarrier_snr >= self.snr_threshold, axis=0))[0]\n",
    "\n",
    "        if len(good_subcarriers) == 0:\n",
    "            self.subcarrier_power = subcarrier_power\n",
    "            return 0\n",
    "\n",
    "        # Filter out the subcarriers with low SNR\n",
    "        filtered_csi_data = subcarrier_power[:, good_subcarriers]\n",
    "\n",
    "        # find the best subcarrier\n",
    "        best_subcarrier = np.argmax(np.mean(filtered_csi_data, axis=0))\n",
    "\n",
    "        # print(\"Best subcarrier:\", best_subcarrier)\n",
    "        \n",
    "        # print(\"Original CSI data size:\", subcarrier_power.shape)\n",
    "        # print(\"Filtered CSI data size:\", filtered_csi_data.shape)\n",
    "        self.subcarrier_power = filtered_csi_data\n",
    "\n",
    "        return best_subcarrier\n",
    "\n",
    "    def predict(self, csi_data):\n",
    "        # Filter out the subcarriers with low SNR\n",
    "        best_subcarrier = self.preprocess(csi_data)\n",
    "        # print(\"Best subcarrier:\", best_subcarrier)\n",
    "\n",
    "        # # Compute the motion statistics for each subcarrier\n",
    "        # motion_statistics = motion_detection_score_all(filtered_CSI_amplitude)\n",
    "\n",
    "        # # Find the subcarrier with the highest motion statistics\n",
    "        # best_subcarrier = np.argmax(motion_statistics)\n",
    "\n",
    "        # Compute the motion statistics for the best subcarrier\n",
    "        motion_statistics = self.motion_detection_score_sub1(subcarrier=best_subcarrier)\n",
    "\n",
    "        if motion_statistics >= self.motion_threshold:\n",
    "            return True\n",
    "        \n",
    "        \n",
    "        autocorr = lowpass_filter(api.tsa.stattools.acf(self.mrc_csi(), nlags=700 - 1)[1:], 1, self.sampling_freq)\n",
    "\n",
    "        autocorr *= -1\n",
    "        # Find the peaks in the motion statistics\n",
    "        peaks, _ = find_peaks(autocorr, prominence=self.prominence, width=self.width)\n",
    "\n",
    "        return len(peaks) >= 2\n",
    "    \n",
    "    \n",
    "def parse_data(data_string):\n",
    "    return data_string[1:-1].split(',')\n",
    "\n",
    "def parse_data2(data_string):\n",
    "    return data_string[1:-1].split(', ')\n",
    "\n",
    "def parse_csi_data(data):\n",
    "    return [complex(int(data[i]), int(data[i+1])) for i in range(0, len(data), 2)]\n",
    "\n",
    "def parse_csi_data2(data):\n",
    "    return [complex(int(data[i][1:-1]), int(data[i+1][1:-1])) for i in range(0, len(data), 2)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv('go_outside/3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = HumanPrescenceClassifier(snr_threshold=0.009, motion_threshold=0.1, prominence=0.011, width=70, sampling_freq=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      "----\n",
      "----\n",
      "----\n",
      "----\n",
      "----\n",
      "----\n",
      "----\n",
      "----\n",
      "(39, 1000, 64)\n",
      "----\n",
      "----\n",
      "----\n",
      "(11, 1000, 64)\n",
      "(50, 1000, 64)\n",
      "(50,)\n"
     ]
    }
   ],
   "source": [
    "# combine the data\n",
    "file_list_positive = ['sitting/2.csv', 'sitting/3.csv', 'sitting/4.csv', 'sitting/csi_data.csv', 'walking/1.csv', 'walking/2.csv', 'walking/3.csv', 'walking/4.csv', 'walking/5.csv'], (parse_data, parse_csi_data), True\n",
    "file_list_negative = ['no one/2.csv', 'no one/3.csv', 'no one/csi_data.csv'], (parse_data2, parse_csi_data2), False\n",
    "CSI_data_list = []\n",
    "labels = []\n",
    "\n",
    "window_size = 1000\n",
    "step_size = 500\n",
    "\n",
    "for file_list, (f1, f2), label in file_list_positive, file_list_negative:\n",
    "    data = []\n",
    "    for file in file_list:\n",
    "        df = pd.read_csv(file)\n",
    "        df['data'] = df['data'].apply(f1)\n",
    "        CSI_data = np.array(df['data'].apply(f2).to_list())\n",
    "\n",
    "        for i in range(window_size, CSI_data.shape[0], step_size):\n",
    "            # get the window data (last window size data)\n",
    "            window_CSI_data = CSI_data[max(0, i-window_size):i, :]\n",
    "            \n",
    "            # predict\n",
    "            data.append(window_CSI_data)\n",
    "        print(\"----\")\n",
    "\n",
    "    data = np.array(data[:])\n",
    "    CSI_data_list.append(data)\n",
    "    print(data.shape)\n",
    "    # labels.append(np.ones(CSI_data.shape[0]) * label)\n",
    "    labels.append(np.full((data.shape[0],), label, dtype=bool))\n",
    "\n",
    "\n",
    "CSI_data = np.concatenate(CSI_data_list, axis=0)\n",
    "print(CSI_data.shape)\n",
    "labels = np.concatenate(labels, axis=0)\n",
    "print(labels.shape)\n",
    "\n",
    "# CSI_data = np.concatenate(CSI_data_list, axis=0)\n",
    "# print(CSI_data.shape)\n",
    "# labels = np.concatenate(labels, axis=0)\n",
    "# print(labels.shape)\n",
    "\n",
    "\n",
    "# for file_list, (f1, f2), label in file_list_positive, file_list_negative:\n",
    "#     df = None\n",
    "#     for file in file_list:\n",
    "#         if df is None:\n",
    "#             df = pd.read_csv(file)\n",
    "#         else:\n",
    "#             df = df.append(pd.read_csv(file))\n",
    "\n",
    "#         # df = pd.read_csv(file)\n",
    "#     df['data'] = df['data'].apply(f1)\n",
    "#     CSI_data = np.array(df['data'].apply(f2).to_list())\n",
    "#     CSI_data_list.append(CSI_data)\n",
    "#     print(CSI_data.shape)\n",
    "#     # labels.append(np.ones(CSI_data.shape[0]) * label)\n",
    "#     labels.append(np.full((CSI_data.shape[0],), label, dtype=bool))\n",
    "\n",
    "# CSI_data = np.concatenate(CSI_data_list, axis=0)\n",
    "# print(CSI_data.shape)\n",
    "# labels = np.concatenate(labels, axis=0)\n",
    "# print(labels.shape)\n",
    "# window_size = 1000\n",
    "# step_size = 500\n",
    "# for i in range(window_size, CSI_data.shape[0], step_size):\n",
    "#     # get the window data (last window size data)\n",
    "#     window_CSI_data = CSI_data[max(0, i-window_size):i+1, :]\n",
    "    \n",
    "#     # predict\n",
    "#     if not classifier.predict(window_CSI_data):\n",
    "#         print(\"No human prescence detected at index\", i)\n",
    "# print(\"----\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False, False, False, False, False, False, False, False,\n",
       "       False])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from scipy.optimize import minimize\n",
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "# Assume that the CSI data is stored in a numpy array called csi_data.\n",
    "# The CSI data should be a 2D array with shape (num_samples, num_subcarriers).\n",
    "\n",
    "# Split the CSI data into a training set and a validation set\n",
    "X_train, X_val, y_train, y_val = train_test_split(CSI_data, labels, test_size=.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15, 1000, 64)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15, 1000, 64)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = HumanPrescenceClassifier(snr_threshold=0.1, motion_threshold=0.1, prominence=0.011, width=70, sampling_freq=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "classifier = HumanPrescenceClassifier(**{'motion_threshold': 0.27507400608839916, 'prominence': 0.021202887303199747, 'snr_threshold': 0.035990422909134456, 'width': 34.657920510962626})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def report_metrics(y_true, model, X):\n",
    "    # predict valid set\n",
    "    predictions = []\n",
    "\n",
    "    for i in range(X.shape[0]):\n",
    "        # get the window data (last window size data)\n",
    "        window_CSI_data = X[i]\n",
    "        \n",
    "        # predict\n",
    "        predictions.append(model.predict(window_CSI_data))\n",
    "\n",
    "    # report the tpr, fpr, tnr, fnr\n",
    "    print(\"TPR:\", np.sum(np.logical_and(predictions, y_true)) / np.sum(y_true))\n",
    "    print(\"FPR:\", np.sum(np.logical_and(predictions, np.logical_not(y_true))) / np.sum(np.logical_not(y_true)))\n",
    "    print(\"TNR:\", np.sum(np.logical_and(np.logical_not(predictions), np.logical_not(y_true))) / np.sum(np.logical_not(y_true)))\n",
    "    print(\"FNR:\", np.sum(np.logical_and(np.logical_not(predictions), y_true)) / np.sum(y_true))\n",
    "\n",
    "    print(\"Accuracy:\", accuracy_score(y_true, predictions))\n",
    "    print(\"Precision:\", precision_score(y_true, predictions))\n",
    "    print(\"Recall:\", recall_score(y_true, predictions))\n",
    "    print(\"F1:\", f1_score(y_true, predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_classifier = HumanPrescenceClassifier(snr_threshold=0.1, motion_threshold=0.1, prominence=0.011, width=70, sampling_freq=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TPR: 0.9166666666666666\n",
      "FPR: 0.0\n",
      "TNR: 1.0\n",
      "FNR: 0.08333333333333333\n",
      "Accuracy: 0.9333333333333333\n",
      "Precision: 1.0\n",
      "Recall: 0.9166666666666666\n",
      "F1: 0.9565217391304348\n"
     ]
    }
   ],
   "source": [
    "report_metrics(y_val, best_classifier, X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TPR: 0.9230769230769231\n",
      "FPR: 0.0\n",
      "TNR: 1.0\n",
      "FNR: 0.07692307692307693\n",
      "Accuracy: 0.94\n",
      "Precision: 1.0\n",
      "Recall: 0.9230769230769231\n",
      "F1: 0.9600000000000001\n"
     ]
    }
   ],
   "source": [
    "report_metrics(labels, best_classifier, CSI_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | motion... | promin... | snr_th... |   width   |\n",
      "-------------------------------------------------------------------------\n",
      "| \u001b[0m1        \u001b[0m | \u001b[0m0.8261   \u001b[0m | \u001b[0m0.417    \u001b[0m | \u001b[0m0.7203   \u001b[0m | \u001b[0m0.0001144\u001b[0m | \u001b[0m30.23    \u001b[0m |\n",
      "| \u001b[95m2        \u001b[0m | \u001b[95m0.875    \u001b[0m | \u001b[95m0.1468   \u001b[0m | \u001b[95m0.09234  \u001b[0m | \u001b[95m0.1863   \u001b[0m | \u001b[95m34.56    \u001b[0m |\n",
      "| \u001b[0m3        \u001b[0m | \u001b[0m0.8511   \u001b[0m | \u001b[0m0.3968   \u001b[0m | \u001b[0m0.5388   \u001b[0m | \u001b[0m0.4192   \u001b[0m | \u001b[0m68.52    \u001b[0m |\n",
      "| \u001b[0m4        \u001b[0m | \u001b[0m0.875    \u001b[0m | \u001b[0m0.2045   \u001b[0m | \u001b[0m0.8781   \u001b[0m | \u001b[0m0.02739  \u001b[0m | \u001b[0m67.05    \u001b[0m |\n",
      "| \u001b[0m5        \u001b[0m | \u001b[0m0.8261   \u001b[0m | \u001b[0m0.4173   \u001b[0m | \u001b[0m0.5587   \u001b[0m | \u001b[0m0.1404   \u001b[0m | \u001b[0m19.81    \u001b[0m |\n",
      "| \u001b[0m6        \u001b[0m | \u001b[0m0.3636   \u001b[0m | \u001b[0m0.8007   \u001b[0m | \u001b[0m0.9683   \u001b[0m | \u001b[0m0.3134   \u001b[0m | \u001b[0m69.23    \u001b[0m |\n",
      "| \u001b[0m7        \u001b[0m | \u001b[0m0.1379   \u001b[0m | \u001b[0m0.8764   \u001b[0m | \u001b[0m0.8946   \u001b[0m | \u001b[0m0.08504  \u001b[0m | \u001b[0m3.905    \u001b[0m |\n",
      "| \u001b[0m8        \u001b[0m | \u001b[0m0.875    \u001b[0m | \u001b[0m0.1698   \u001b[0m | \u001b[0m0.8781   \u001b[0m | \u001b[0m0.09835  \u001b[0m | \u001b[0m42.11    \u001b[0m |\n",
      "| \u001b[0m9        \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m0.9579   \u001b[0m | \u001b[0m0.5332   \u001b[0m | \u001b[0m0.6919   \u001b[0m | \u001b[0m31.55    \u001b[0m |\n",
      "| \u001b[0m10       \u001b[0m | \u001b[0m0.7442   \u001b[0m | \u001b[0m0.6865   \u001b[0m | \u001b[0m0.8346   \u001b[0m | \u001b[0m0.01829  \u001b[0m | \u001b[0m75.01    \u001b[0m |\n",
      "| \u001b[0m11       \u001b[0m | \u001b[0m0.871    \u001b[0m | \u001b[0m0.00772  \u001b[0m | \u001b[0m0.1619   \u001b[0m | \u001b[0m0.7094   \u001b[0m | \u001b[0m76.47    \u001b[0m |\n",
      "| \u001b[0m12       \u001b[0m | \u001b[0m0.8261   \u001b[0m | \u001b[0m0.4426   \u001b[0m | \u001b[0m0.4135   \u001b[0m | \u001b[0m0.5478   \u001b[0m | \u001b[0m74.42    \u001b[0m |\n",
      "| \u001b[0m13       \u001b[0m | \u001b[0m0.8      \u001b[0m | \u001b[0m0.4883   \u001b[0m | \u001b[0m0.314    \u001b[0m | \u001b[0m0.5635   \u001b[0m | \u001b[0m74.46    \u001b[0m |\n",
      "| \u001b[0m14       \u001b[0m | \u001b[0m0.871    \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m0.08747  \u001b[0m | \u001b[0m0.5167   \u001b[0m | \u001b[0m67.69    \u001b[0m |\n",
      "| \u001b[0m15       \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m0.9224   \u001b[0m | \u001b[0m0.8844   \u001b[0m | \u001b[0m0.9051   \u001b[0m | \u001b[0m67.55    \u001b[0m |\n",
      "| \u001b[0m16       \u001b[0m | \u001b[0m0.875    \u001b[0m | \u001b[0m0.1149   \u001b[0m | \u001b[0m0.4185   \u001b[0m | \u001b[0m0.02464  \u001b[0m | \u001b[0m68.1     \u001b[0m |\n",
      "| \u001b[0m17       \u001b[0m | \u001b[0m0.871    \u001b[0m | \u001b[0m0.0009524\u001b[0m | \u001b[0m0.07948  \u001b[0m | \u001b[0m0.1044   \u001b[0m | \u001b[0m66.74    \u001b[0m |\n",
      "| \u001b[0m18       \u001b[0m | \u001b[0m0.871    \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m0.7693   \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m75.53    \u001b[0m |\n",
      "| \u001b[0m19       \u001b[0m | \u001b[0m0.5789   \u001b[0m | \u001b[0m0.7679   \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m76.45    \u001b[0m |\n",
      "| \u001b[0m20       \u001b[0m | \u001b[0m0.875    \u001b[0m | \u001b[0m0.2584   \u001b[0m | \u001b[0m0.6573   \u001b[0m | \u001b[0m0.04691  \u001b[0m | \u001b[0m65.82    \u001b[0m |\n",
      "| \u001b[0m21       \u001b[0m | \u001b[0m0.871    \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m0.02967  \u001b[0m | \u001b[0m35.66    \u001b[0m |\n",
      "| \u001b[95m22       \u001b[0m | \u001b[95m0.9057   \u001b[0m | \u001b[95m0.06341  \u001b[0m | \u001b[95m0.6879   \u001b[0m | \u001b[95m0.8965   \u001b[0m | \u001b[95m35.28    \u001b[0m |\n",
      "| \u001b[0m23       \u001b[0m | \u001b[0m0.5405   \u001b[0m | \u001b[0m0.7666   \u001b[0m | \u001b[0m0.9075   \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m35.25    \u001b[0m |\n",
      "| \u001b[0m24       \u001b[0m | \u001b[0m0.871    \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m0.6019   \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m34.23    \u001b[0m |\n",
      "| \u001b[0m25       \u001b[0m | \u001b[0m0.8511   \u001b[0m | \u001b[0m0.4451   \u001b[0m | \u001b[0m0.06393  \u001b[0m | \u001b[0m0.9265   \u001b[0m | \u001b[0m36.02    \u001b[0m |\n",
      "=========================================================================\n",
      "Optimal thresholds: {'motion_threshold': 0.06341187482433774, 'prominence': 0.6878931999176817, 'snr_threshold': 0.8964863630318453, 'width': 35.277702134032374}\n",
      "Maximum accuracy_score score: 0.9056603773584906\n",
      "Validation accuracy_score: 0.9166666666666666\n"
     ]
    }
   ],
   "source": [
    "# Split the data into training and validation sets\n",
    "# X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "\n",
    "# Define the function to optimize, which takes two threshold values and returns the F1 score\n",
    "def objective(snr_threshold, motion_threshold, prominence, width):\n",
    "    predictions = []\n",
    "    classifier = HumanPrescenceClassifier(snr_threshold=snr_threshold, motion_threshold=motion_threshold, prominence=prominence, width=width, sampling_freq=100)\n",
    "    for i in range(X_train.shape[0]):\n",
    "        # get the window data (last window size data)\n",
    "        window_CSI_data = X_train[i]\n",
    "        \n",
    "        # predict\n",
    "        predictions.append(classifier.predict(window_CSI_data))\n",
    "\n",
    "    score = f1_score(y_train, predictions)  # Compute the F1 score\n",
    "    return score  # Minimize the negative F1 score\n",
    "\n",
    "# Set the bounds of the search space for the thresholds\n",
    "pbounds = {'snr_threshold': (0, 1), 'motion_threshold': (0, 1), 'prominence': (0, 1), 'width': (0, 100)}\n",
    "\n",
    "# Use a Bayesian optimization algorithm to find the optimal thresholds\n",
    "optimizer = BayesianOptimization(\n",
    "    f=objective,\n",
    "    pbounds=pbounds,\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "optimizer.maximize(\n",
    "    init_points=10,\n",
    "    n_iter=15,\n",
    ")\n",
    "\n",
    "# Print the optimal thresholds and the maximum F1 score\n",
    "print('Optimal thresholds:', optimizer.max['params'])\n",
    "print('Maximum accuracy_score score:', optimizer.max['target'])\n",
    "classifier = HumanPrescenceClassifier(**optimizer.max['params'])\n",
    "# predict valid set\n",
    "predictions = []\n",
    "\n",
    "for i in range(X_val.shape[0]):\n",
    "    # get the window data (last window size data)\n",
    "    window_CSI_data = X_val[i]\n",
    "    \n",
    "    # predict\n",
    "    predictions.append(classifier.predict(window_CSI_data))\n",
    "print(\"Validation accuracy_score:\", f1_score(y_val, predictions))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
